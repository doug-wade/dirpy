#!/platform/python/bin/python

__version__ = "0.1"

import os, sys, datetime, time, ConfigParser, argparse, logging, signal
import multiprocessing, socket, BaseHTTPServer, urllib, urllib2, urlparse
import cStringIO, threading

# Gracefully exit if PIL is missing
try:
    from PIL import Image, ImageFile
except:
    print ("Missing the PIL module; please consult the README.md " +
        "file for instructions on how to install PIL.")
    sys.exit(1)

# Workaround for truncated images
ImageFile.LOAD_TRUNCATED_IMAGES = True

# Our dirpy function.  This is where all the heavy lifting is done ############
def dirpyWorker(req, head=False): #############################################

    # Parse our query string into a dict
    parsedPath = urlparse.urlparse(req.path)
    path = parsedPath.path
    query = urlparse.parse_qs(
        urllib.unquote(parsedPath.query).decode('utf-8').lower(),
        keep_blank_values=True)
    for key in query:
        query[key] = query[key][0]

    # Make sure we have a (valid) command
    if "cmd" not in query:
        logger.error("Missing Dirpy command")
        return req.send_error(400, "Missing Dirpy command")

    cmd = query["cmd"]
    if cmd not in ("shrink","enlarge","resize","recompress"):
        logger.error("Invalid Dirpy command: %s" % cmd)
        return req.send_error(400, "Invalid Dirpy command")

    # Make sure we have at least a width or a height if we are
    # doing any sort of transformation.  We store these as 'reqX' and
    # 'reqY', respectively (i.e. the "requested" dimensions)
    try:
        reqX = int(query["width"]) if "width" in query else None
        reqY = int(query["height"]) if "height" in query else None
    except:
        return req.send_error(400, "Height and width must be numeric")
    if cmd != "recompress" and not (reqX or reqY):
        return req.send_error(400, "Need height and/or width for " + cmd)

    # Set our boolean operators
    distort     = "distort" in query
    crop        = "crop" in query
    pad         = "pad" in query
    isMax       = "max" in query
    optimize    = "optimize" in query
    progressive = "progressive" in query
    noicc       = "noicc" in query

    # Handle mutually exclusive options
    if crop + distort + pad + isMax > 1:
        return req.send_error(400, "Distort/crop/pad/max mutually exclusive")

    # If we are distorting (not obeying aspect ratio) or cropping,
    # then we have to have both height AND width
    if (distort or crop or pad or isMax) and not (reqX and reqY):
        return req.send_error(400, 
            "Need both height and width for distort/crop/pad/max")

    # Set our resampling filter
    filterName = query['filter'] if 'filter' in query else None
    if filterName == 'bilinear':
        filter = Image.BILINEAR
    elif filterName == 'bicubic':
        filter = Image.BICUBIC
    elif filterName == 'nearest':
        filter = Image.NEAREST
    else:
        filterName = 'antialias';
        filter = Image.ANTIALIAS

    # Normalize our path (and prevent directory traversal)
    localFile = os.path.normpath(cfg.httpRoot +
        os.path.normpath("/" + path))

    # Determine whether we are proxying or not.  If we are
    # proxying, use urllib2 to open the remote file.
    fileObj = None
    if "proxy" in query and not ("fallback" in query and
            os.path.isfile(localFile)):
        proxyUrl = urllib.unquote(query['proxy']) + path
        filePath = proxyUrl
        logger.debug("Requesting proxy url: %s" % proxyUrl)
        try:
            proxyReq = urllib2.Request(proxyUrl,
                headers={"User-Agent": "Dirpy/" + __version__})
            proxyRes = urllib2.urlopen(proxyReq)
            fileObj = cStringIO.StringIO(proxyRes.read())
        except Exception as e:
            proxyCode = e.code or 500
            logger.info("Proxy error; url: '%s', error: '%s'" %
                (proxyUrl, e))
            return req.send_error(proxyCode, "Proxy error: %s" % e)

    # If we're not proxying, look for the file locally
    else:
        filePath = localFile
        logger.debug("Serving local file: %s" % filePath)

        # Try to open our file
        try:
            fileObj = open(filePath, "rb")
        except Exception as e:
            logger.info("Error opening local file '%s': %s" %
                (filePath, e))
            return req.send_error(404, "Access Denied")

    # Process the image
    try:
        imIn = Image.open(fileObj)
        # Guard against decompression bombs
        if cfg.maxPixels and imIn.size[0] * imIn.size[1] > cfg.maxPixels:
            raise Exception("Image exceeds maximum pixel count")
    except Exception as e:
        logger.info("Error opening '%s' as image: %s" % (filePath, e))
        return req.send_error(500, "Error opening image file")

    # Set and spot-check our input & output formats
    inFmt = imIn.format.lower()

    if 'format' in query:
        outFmt = query['format']
    elif inFmt:
        outFmt = inFmt
    else:
        logger.debug("Can't determine encoder; falling back to jpeg")
        outFmt = 'jpg' # Fall back to jpeg


    # Preserve aspect ratio unless told not to do so.  Also
    # fill in missing height/width values
    origX, origY = imIn.size
    newX, newY = reqX, reqY
    aspect=None
    if cmd != "recompress" and not distort:
        if not newY:
            resizeRatio = float(newX)/origX
        elif not newX:
            resizeRatio = float(newY)/origY
        else:
            if crop or isMax:
                if crop:
                    aspect="crop"
                else:
                    aspect="max"
                resizeRatio = max(float(newX)/origX, float(newY)/origY)
            else:
                resizeRatio = min(float(newX)/origX, float(newY)/origY)

        # Re-evaluate our target dimentions to preserve aspect ratio
        newX = int(origX * resizeRatio)
        newY = int(origY * resizeRatio)

        # Use draft size hinting to considerably speed things up
        if newX * newY < origX * origY:
            imIn.draft(None,(newX,newY))

    # If we are doing a distort resize, we don't need to worry about
    # doing aspect ratio calculations, since we just resize to the
    # requestion dimensions regardless of aspect ratios
    elif cmd != "recompress" and distort:
        aspect="distort"
        if reqX * reqY < origX * origY:
            imIn.draft(None,(reqX,reqY))

    # Send some detailed debug info
    if not aspect:
        if pad:
            aspect="pad"
        else:
            aspect="preserve"

    logger.debug(("Processing %s: cmd=%s origX=%s origY=%s " +
        "reqX=%s reqY=%s aspect=%s filter=%s optimize=%s " +
        "progressive=%s inFmt=%s outFmt=%s") % 
        (filePath, cmd, origX, origY, reqX, reqY, aspect,
        filterName, optimize, progressive, inFmt, outFmt))

    # Load the source image
    try:
        imIn.load()
    except Exception as e:
        logger.error("Error loading image %s: %s" % (filePath,e))
        return req.send_error(500, "Error loading image")


    # Process resize commands, honoring shrink and enlarge constraints
    if cmd in ("shrink","enlarge","resize"):
        if ((not crop and
                (cmd == "shrink" and (newX > origX or newY > origY) or
                cmd == "enlarge" and (newX < origX or newY < origY))) or
           (crop and
                (cmd == "shrink" and newX > origX and newY > origY or
                cmd == "enlarge" and newX < origX and newY < origY))):
            imOut = imIn
        else:
            imOut = imIn.resize((newX, newY), filter)

        # Do image padding (to make the output image match the
        # requested size). Don't bother if the image already
        # matches the required size
        if pad and (reqX != newX or reqY != newY):
            padColor = query["pad"]
            padMode = imOut.mode

            # If we have a transparent background color selected and
            # we arent in alpha-channel RGB mode, switch to that mode
            if padColor == "transparent":
                if padMode != "RGBA":
                    padMode = "RGBA"

                # Convert our transparency string to an alpha channel
                padColor = (255,255,255,255)

            # If we aren't in RGB mode and someone passes a non-int
            # color, revert to RGB mode
            if padMode[:3] != "RGB":
                try:
                    padColor = int(padColor)
                except:
                    padMode = "RGB"

            # Now embed the resized image into a new padded image
            # Use center gravity by default, for now.  We might add
            # user-configurable gravities later
            try:
                imPad = Image.new(padMode, (reqX, reqY), padColor)
                imPad.paste(imOut, ((reqX-newX)/2, (reqY-newY)/2))
                imOut = imPad
            except Exception as e:
                logger.error("Error padding image %s: %s" %
                    (filePath,e))
                return req.send_error(500, "Error padding image")

        # Crop our image to the requested size. Once again, don't
        # bother if the image size already matches the requested
        elif crop and (reqX != newX or reqY != newY):
            cropLeft    = (newX-reqX)/2
            cropRight   = reqX+cropLeft
            cropTop     = (newY-reqY)/2
            cropBottom  = reqY+cropTop 

            try:
                imCrop = imOut.crop((cropLeft, cropTop,
                    cropRight, cropBottom))
                imCrop.load()
                imOut = imCrop
            except Exception as e:
                logger.error("Error cropping image: %s" %
                    (filePath, e))
                return req.send_error(500, "Error cropping image")

    # We dont do any image transformations for recompression;
    # just copy the input image to the output image
    elif cmd == "recompress":
       imOut = imIn 

    # Set output quality (only affects jpeg/webp formats)
    if outFmt in ('jpeg', 'webp'):
        if 'quality' in query:
            qualVal = query['quality']
        else:
            qualVal = cfg.defQuality
    else:
        qualVal = None

    # Process image quality adjustments
    if qualVal is not None:
        try:
            # Set our quality; make sure it is an integer
            qualVal = int(qualVal)
            # Snarf out minimum pixel dimension value
            minRecompressPixels = (int(query['minrecompresspixels']) 
                if 'minrecompresspixels' in query 
                else cfg.minRecompressPixels)

            # Make sure we got a valid quality percentage
            if not 0 < qualVal < 101:
                raise Exception("Invalid quality")

            # Dont recompress input images that are less than this size
            if origX * origY < minRecompressPixels:
                logger.debug("Not recompressing image: %s < %s" % 
                    (imIn.size[0] * imIn.size[1], minRecompressPixels))
                qualVal=95

        except Exception as e:
            logger.error("Failed to set quality to '%s': %s" %
                (qualVal, e))

    # Preserve our ICC profile if this is a JPEG, unless explicitly
    # requested otherwise
    iccProf = None
    if imIn.format.lower() == "jpeg" and not noicc:
        iccProf = imIn.info.get('icc_profile')

    # Now write the converted image to a buffer
    try:
        # set up our stringIO object, since PIL refuses to write
        # directly to a string
        outBuf = cStringIO.StringIO()

        # Our output arguments.  We have to to use a kwargs pointer, since
        # the save function will sometimes interpret the presence of
        # an argument (regardless of its value) to mean a true value

        # Bump up the ImageFile.MAXBLOCK size when writing optimized or
        # progressive images to avoid a legacy PIL bug
        kwargs = {'format' : outFmt}
        if progressive or optimize:
            ImageFile.MAXBLOCK =  max(imIn.size[0] * imIn.size[1], 
                imOut.size[0] * imOut.size[1], 2097152)
            logger.debug("MAXBLOCK set to: %s" % ImageFile.MAXBLOCK)
        if optimize: 
            kwargs["optimize"] = True
        if progressive: 
            kwargs["progressive"] = True
        if iccProf is not None: 
            kwargs["icc_profile"] = iccProf
        if qualVal is not None:
            kwargs["quality"] = qualVal

        # Save our image to the strinIO buffer, with all of our
        # various user-defined or default config options
        # Note that any "failed to suspend" errors here are typically
        # caused by your MAXBLOCK variable being too small
        try:
            imOut.save(outBuf, **kwargs)
        except Exception as e:
            logger.error("Failed to save image: %s" % e)
            raise Exception("Failed to execute save()")


        # See to the end of the buffer so we can get our content
        # size without allocating to a string (which we don't want
        # to do if this is a HEAD request).  Then seek back to the 
        # beginning so we can read the string later
        outBuf.seek(0,os.SEEK_END)
        outSize = outBuf.tell()
        outBuf.seek(0)

    except Exception as e:
        logger.error("Error converting image '%s': %s" % 
            (filePath, e))
        return req.send_error(500, "Error converting image")

    logger.debug(
        "Done processing %s: quality=%s, size=%s, dim=%sx%s" % 
        ((filePath,qualVal,outSize)+imOut.size))

    # Now fire off a response to our client
    req.send_response(200)
    req.send_header("Content-Type", "image/%s" % outFmt)
    req.send_header("Content-Length", outSize)
    req.end_headers()

    # Don't send actual data if this is a HEAD request
    if head:
        return

    # Guard against a broken TCP connection raising an exception
    # by wrapping the output buffer read/write loop in a try block
    try:
        while True:
            buf = outBuf.read(4096)
            if not buf:
                break
            req.wfile.write(buf)
    except:
        pass

    return



# Read in the command line and file based configuration parameters
def readConfig(): ##########################################################

    # Build our config parser
    parser = argparse.ArgumentParser(
        description="DIRPY: the Dynamic Image Resizing Program, Yay!")
    parser.add_argument("-c", "--config-file", default="/etc/dirpy.conf",
        help="Path to the Dirpy config file")
    parser.add_argument("-d", "--debug", action="store_true",
        help="Emit debug output")
    parser.add_argument("-f", "--foreground", action="store_true",
        help="Don't daemonize; run program in the foreground")

    # Parse command line
    global cfg
    cfg = parser.parse_args()

    # Open our config file
    cfgParser = ConfigParser.RawConfigParser()
    try:
        if not cfgParser.read(cfg.config_file):
            cfg.defaults = True
        else:
            cfg.defaults = False
    except Exception as e:
        fatal("Unable to load config file '%s': %s" % (cfg.config_file, e))

    # Read in all of our global / default options
    cfg.pidFile     = cfgStr(cfgParser,
        'global', 'pidFile', False, '/var/run/dirpy/dirpy.pid')
    cfg.logFile     = cfgStr(cfgParser,
        'global', 'logFile', False, '/var/log/dirpy.log')
    cfg.bindAddr    = cfgStr(cfgParser,
        'global', 'bindAddr', False,  '0.0.0.0')
    cfg.bindPort    = cfgInt(cfgParser,
        'global', 'bindPort', False,  3000)
    cfg.httpRoot    = cfgStr(cfgParser,
        'global', 'httpRoot', False,  '/var/www/html')
    cfg.numWorkers  = cfgInt(cfgParser,
        'global', 'numworkers', False,  multiprocessing.cpu_count()*2)
    cfg.maxPixels   = cfgInt(cfgParser,
        'global', 'maxPixels', False,  90000000)
    cfg.defQuality  = cfgInt(cfgParser,
        'global', 'defQuality', False,  95)
    cfg.minRecompressPixels  = cfgInt(cfgParser,
        'global', 'minRecompressPixels', False,  0)
    cfg.reqTimeout = cfgInt(cfgParser,
        'global', 'reqTimeout', False, None)


# Grab an string from our config
def cfgStr(cfg, section, name, required=True, default=None): #################
    try:
        return cfg.get(section, name)
    except ConfigParser.Error:
        if not required:
            return default
        fatal("Missing required config parameter %s:%s." % (section, name))


# Grab an int from our config, complain if it isn't valid
def cfgInt(cfg, section, name, required=True, default=None): ################
    try:
        # Allow a string if they match the default value
        return cfg.getint(section, name)
    except ValueError:
        fatal("Config parameter %s:%s must be an integer." % (section, name))
    except ConfigParser.Error:
        if not required:
            return default
        fatal("Missing required config parameter %s:%s." % (section, name))


# Grab an network address from our config, complain if it isn't valid
def cfgAddr(cfg, section, name, required=True, default=None): ################
    # Fetch and validate a hostname/ip address config option
    try:
        addr = cfg.get(section, name)
        socket.gethostbyname(addr)
        return addr
    except ConfigParser.Error:
        if not required:
            return default
        fatal("Missing required config parameter %s:%s." % (section, name))
    except socket.gaierror:
        fatal("Invalid address for config parameter %s:%s" % (section, name))


# Class based on WatchedFileHandler, but which is more tolerant of
# non-existant files (i.e. those that might be in the middle of log rotation)
class TolerantFileHandler(logging.FileHandler): ###############################

    # Make note if our log file, it's device and it's inode
    def __init__(self, filename, mode='a', encoding=None, delay=0):
        logging.FileHandler.__init__(self, filename, mode, encoding, delay)

        try:
            stat = os.stat(self.baseFilename)
            self.dev, self.ino = stat[ST_DEV], stat[ST_INO]
        except OSError:
            self.dev, self.ino = -1, -1

    # Check if the file has changed; if so try and re-open it
    def emit(self, record):

        try:
            stat = os.stat(self.baseFilename)
            changed = (stat[ST_DEV] != self.dev) or (stat[ST_INO] != self.ino)
        except OSError:
            stat = None
            changed = 1

        if changed and self.stream is not None:
            # Flushing and closing our old stream are optional, in case the
            # file is no longer writable (or it has been deleted)
            try:
                self.stream.flush()
                self.stream.close()
            except Exception:
                pass

            self.stream = self._open()
            if stat is None:
                stat = os.stat(self.baseFilename)
            self.dev, self.ino = stat[ST_DEV], stat[ST_INO]
        logging.FileHandler.emit(self, record)


# Class to allow appropriate multiprocess logging
class MultiProcessingLog(logging.Handler): ###################################

    # Create a queue-based log handler, which in turn will call another
    # log handler as specified by the init.  
    def __init__(self, subHandler):
        logging.Handler.__init__(self)

        self._handler = subHandler
        self.queue = multiprocessing.Queue(-1)

        t = threading.Thread(target=self.receive)
        t.daemon = True
        t.start()

    # Handle custom formats; make sure our sub-formatter gets the same format
    # spec
    def setFormatter(self, fmt):
        logging.Handler.setFormatter(self, fmt)
        self._handler.setFormatter(fmt)

    # Pop a message off our queue and emit it, handle sig-int's appropriately
    def receive(self):
        while True:
            try:
                record = self.queue.get()
                self._handler.emit(record)
            except (KeyboardInterrupt, SystemExit):
                raise
            except EOFError:
                break
            except:
                traceback.print_exc(file=sys.stderr)

    # Enqueue a message for eventual emission
    def send(self, s):
        self.queue.put_nowait(s)

    # Format our record according to our format spec; handle stack traces
    # appropriately
    def _format_record(self, record):
        if record.args:
            record.msg = record.msg % record.args
            record.args = None
        if record.exc_info:
            dummy = self.format(record)
            record.exc_info = None

        return record

    # Format a send a message
    def emit(self, record):
        try:
            s = self._format_record(record)
            self.send(s)
        except (KeyboardInterrupt, SystemExit):
            raise
        except:
            self.handleError(record)

    # Gracefully close our subhandler and ourself
    def close(self):
        self._handler.close()
        logging.Handler.close(self)


# Set up our global logger
def loggerSetup(): ###########################################################


    # Set our maximum severity level to log (i.e. debug or not)
    logLevel = logging.DEBUG if cfg.debug else logging.INFO

    # Log configuration
    global logger

    # Log to stdout if we are running in the foreground
    if cfg.foreground:
        subHandler = logging.StreamHandler(sys.stdout)

    # If we are running as a daemon, log to our log file
    else:
        if cfg.logFile == "syslog":
            subHandler = logging.handlers.SysLogHandler()
        else:
            subHandler = TolerantFileHandler(cfg.logFile)

    handler = MultiProcessingLog(subHandler)

    try:
        logger = logging.getLogger('dirpy')
        logger.setLevel(logLevel)
        handler.setFormatter(logging.Formatter(
            '%(asctime)s %(levelname)s: %(message)s',
            '[%Y-%m-%d@%H:%M:%S]'
        ))
        logger.addHandler(handler)

    except IOError as e:
        fatal("Unable to log to %s (%S)" % (cfg.logFile, e.strerror))


    # Make the logger emit all unhandled exceptions
    sys.excepthook = lambda t, v, x: logger.exception(str(v))

    if cfg.defaults:
        logger.info("Can't read config file %s; using default values" %
            cfg.config_file)


# Throw a fatal message and exit
def fatal(msg): ##############################################################

    try:
        logger
    # If our logger isnt defined, print directly to stdout
    except:
        ts = datetime.datetime.now().strftime("%Y-%m-%d@%H:%M:%S")
        print "[%s] CRITICAL: %s" % (ts, msg)
    # Otherwise, just use the logger
    else:
        logger.critical(msg)

    sys.exit(1)


# Launch our process as a daemon
def daemonize(): #############################################################

    UMASK = 0
    MAXFD = 1024

    # Fork once
    try:
        pid = os.fork()
    except OSError, e:
        fatal("Unable to fork: %s [%d]" % (e.strerror, e.errno))

    # In the first child process
    if (pid == 0):
        os.setsid()

        try:
            pid = os.fork()
        except OSError, e:
            fatal("Unable to fork: %s [%d]" % (e.strerror, e.errno))

        if (pid == 0):
            os.chdir("/")
            os.umask(UMASK)
        else:
            os._exit(0)
    else:
        os._exit(0)

    # Close all open file descriptors
    for fd in range(0, MAXFD):
        try:
            os.close(fd)
        except OSError:
            pass

    # DUP our stdout & stderr filehandles to dev null
    os.open(os.devnull, os.O_RDWR)
    os.dup2(0, 1)
    os.dup2(0, 2)

    return


# Our HTTP Request handler class
class httpHandler(BaseHTTPServer.BaseHTTPRequestHandler): ####################

    server_version = "Dirpy/" + __version__
    protocol_version = "HTTP/1.1"

    # Override log output (to stdout) and pass to our logger instance
    def log_message(self, format, *args):
        logger.debug('[%s] %s' % (
            multiprocessing.current_process().name, format % args))

    # Direct GET queries to
    def do_GET(self):
        dirpyWorker(self)

    # Direct GET queries to
    def do_HEAD(self):
        dirpyWorker(self, head=True)
        

# Our webserver class.  Implements timeouts
class httpServerTimeout(BaseHTTPServer.HTTPServer): ##########################

    # Extend the HTTPServer constructor, so we can grab our timeout at init
    def __init__(self, server, handler, timeout=None):
        self.timeout = timeout
        BaseHTTPServer.HTTPServer.__init__(self, server, handler)

    # Bind our server and set our socket timeout before we accept connects
    def server_bind(self):
        try:
            BaseHTTPServer.HTTPServer.server_bind(self)
            self.socket.settimeout(self.timeout)
            self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        except Exception as e:
            fatal("Failed to bind server: %s" % e)


# Spawn a worker process, along with the time that it was started
def spawnWorker(target, args): ##############################################

    # Try three times to start a worker, and then give up
    attempts = 3
    while attempts > 0:
        try:
            worker = multiprocessing.Process(target=target, args=args)
            worker.daemon = True
            worker.start()
            return [worker, time.time()]

        # Sad lack of Python documentation for multiprocessing exceptions...
        except Exception as e:
            attempts -= 1
            logger.info("Failed to worker (%s); %s more attempt(s)" %
                (e, attempts))
            time.sleep(1)

    # Uh oh, can't spawn a worker.  Time to shut down 
    fatal("Unable to spawn worker after %s attempts" % attempts)


# The serve_forever wrapper, called by multiprocessing.Process
def serverWrapper(server):
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        pass


# Our main loop
def main(): ##################################################################

    # Read command line parameters and config file
    readConfig()

    # Daemonize, unless requested otherwise
    if not cfg.foreground:
        daemonize()

    # Catch SIGINTs
    signal.signal(signal.SIGINT, lambda s, f: sys.exit())

    # Start our logger
    loggerSetup()

    # Drop our pid file if we are daemonized
    if not cfg.foreground:
        with open(cfg.pidFile, 'w') as fh:
            fh.write(str(os.getpid()))

    # Initialize our http server class
    httpServer = httpServerTimeout(
        (cfg.bindAddr, cfg.bindPort), httpHandler, cfg.reqTimeout)

    # Start our worker pool.  We dont use multiprocessing.pool, since we want
    # to be able to watchdog our server processes
    workers = []
    for i in range(cfg.numWorkers):
        workers.append(spawnWorker(serverWrapper, (httpServer,)))

    # We're up and running; let the world know about it
    logger.info("Dirpy daemon started! Herp da dirp!")
    logger.info("Listing on %s:%s, using %s worker(s) " %
        (cfg.bindAddr, cfg.bindPort, cfg.numWorkers))

    # Enter watchdog mode
    while True:
        time.sleep(1)
        
        # Check to see if any workers have died/exited unexpectedly
        for i in range(cfg.numWorkers):

            # If a worker exited; clean up after it and then restart it
            if not workers[i][0].is_alive():
                logger.error("Worker %s died; restarting it." % i+1)
                workers[i][0].join()
                workers[i] = spawnWorker(serverWrapper, (httpServer,))

    # Shouldn't ever get this far, but just in case...
    sys.exit(1)

if __name__ == "__main__":
    main()
